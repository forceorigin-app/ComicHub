#!/usr/bin/env python3
"""
é€šç”¨æ¼«ç”»ä¸‹è½½å™¨ (ä¼˜åŒ–ç‰ˆ)
- é€»è¾‘ä¿®æ­£ï¼šç²¾ç¡®è§£æ <div class="chapter-list">
- ä¼˜åŒ–ï¼šåªä¸‹è½½æ ¼å¼æœ€ä¼˜çš„å›¾ç‰‡ (ä¼˜å…ˆ webpï¼Œå»é‡)
"""
import sys
import os
import re
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import requests
from bs4 import BeautifulSoup

from comichub.core.fetcher import ManhuaGuiFetcherSelenium

DEFAULT_SAVE_DIR = Path.home() / "data" / "comic"

def get_info(url: str) -> dict:
    """è§£æä¹¦é¡µï¼Œè·å–ä¹¦åå’Œç« èŠ‚åˆ—è¡¨"""
    headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"}
    
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code != 200:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {resp.status_code}")
            return None
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # 1. ä¹¦å
        book_name = "Unknown_Book"
        h1 = soup.find('h1')
        if h1 and h1.text.strip():
            book_name = h1.text.strip()
        
        # 2. ç« èŠ‚åˆ—è¡¨ (<div class="chapter-list">)
        chapter_div = soup.find('div', class_='chapter-list')
        
        if not chapter_div:
            print("âŒ æœªæ‰¾åˆ° .chapter-list å®¹å™¨")
            return None
        
        links = chapter_div.find_all('a')
        
        if not links:
            print("âŒ ç« èŠ‚åˆ—è¡¨ä¸ºç©º")
            return None
        
        chapters = []
        for a in links:
            href = a.get('href', '')
            if href.startswith('/'):
                full_url = urljoin(url, href)
                title = a.get_text(strip=True)
                if title:
                    chapters.append({
                        'title': title,
                        'url': full_url
                    })
        
        return {
            'book_name': book_name,
            'chapters': chapters
        }
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        return None

def download_latest(url: str, save_dir: Path = DEFAULT_SAVE_DIR):
    """ä¸‹è½½æœ€æ–°ä¸€ç« """
    info = get_info(url)
    
    if not info or not info['chapters']:
        return
    
    book_name = info['book_name']
    chapters = info['chapters']
    
    # å–æœ€æ–°ï¼ˆç¬¬ä¸€ä¸ªï¼‰
    latest = chapters[0]
    ch_title = latest['title']
    ch_url = latest['url']
    
    # æ¸…ç†æ–‡ä»¶å
    book_name = re.sub(r'[\\/:*?"<>|]', '', book_name)
    ch_title = re.sub(r'[\\/:*?"<>|]', '', ch_title)
    
    print(f"ğŸ“š ä¹¦å: {book_name}")
    print(f"ğŸ“– ç« èŠ‚: {ch_title}")
    
    # åˆ›å»ºç›®å½•
    ch_path = save_dir / book_name / ch_title
    ch_path.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {ch_path}")
    
    # è·å–å›¾ç‰‡
    print("ğŸš€ å¯åŠ¨ Selenium...")
    fetcher = ManhuaGuiFetcherSelenium()
    images = fetcher.get_images(ch_url)
    fetcher.close()
    
    if not images:
        print("âŒ æœªæ‰¾åˆ°å›¾ç‰‡")
        return
    
    print(f"ğŸ–¼ï¸ æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
    
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": url
    }
    
    success = 0
    processed_urls = set() # ç”¨äºå»é‡ï¼šå¦‚æœä¸€å¼ å›¾ç‰‡åŒæ—¶æœ‰ jpg å’Œ webp é“¾æ¥ï¼Œåªä¸‹è½½ä¸€æ¬¡
    
    for i, img_url in enumerate(images, 1):
        # ç®€å•çš„å»é‡é€»è¾‘ï¼šURL ç›¸åŒåˆ™è·³è¿‡
        # æ³¨æ„ï¼šfetcher.get_images è¿”å›çš„åˆ—è¡¨æœ¬èº«åº”è¯¥æ˜¯ä¸é‡å¤çš„ï¼Œ
        # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬è¿˜æ˜¯åšä¸€æ¬¡ URL æ£€æŸ¥
        if img_url in processed_urls:
            continue
        processed_urls.add(img_url)
        
        # ä¿®å¤åè®®
        if img_url.startswith('//'):
            img_url = 'https:' + img_url
        
        # åˆ¤å®šæ‰©å±•åï¼šä¼˜å…ˆ webp
        ext = '.webp' # é»˜è®¤
        if '.jpg' in img_url.lower(): 
            # å¦‚æœ URL åªæœ‰ jpgï¼Œå°±ç”¨ jpg
            if '.webp' not in img_url.lower():
                ext = '.jpg'
        
        try:
            if i % 5 == 0:
                time.sleep(0.5)
            
            r = requests.get(img_url, headers=headers, stream=True, timeout=15)
            if r.status_code == 200:
                fname = f"{i:03}{ext}"
                fpath = ch_path / fname
                
                with open(fpath, 'wb') as f:
                    for chunk in r.iter_content(8192):
                        f.write(chunk)
                success += 1
                
                print(f"  è¿›åº¦: {success}/{len(images)}", end='\r', flush=True)
        except Exception:
            pass
    
    print(f"\nâœ… å®Œæˆ! æˆåŠŸ: {success}/{len(images)}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python -m scripts.download_comic <ä¹¦é¡µURL>")
        print("ç¤ºä¾‹: python -m scripts.download_comic https://m.manhuagui.com/comic/2592/")
        sys.exit(1)
    
    target = sys.argv[1]
    download_latest(target)
