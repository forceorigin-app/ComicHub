"""
æ¼«ç”»ç« èŠ‚é¡µé¢åˆ†æè„šæœ¬ - æœ€ç»ˆä¿®å¤ç‰ˆï¼ˆä½¿ç”¨æ­£ç¡®çš„åŸŸåï¼‰
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import re
import time
from pathlib import Path
from urllib.parse import urljoin
import logging
import sys
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)

logger = logging.getLogger(__name__)


class ChapterAnalyzer:
    """ç« èŠ‚é¡µé¢åˆ†æå™¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„åŸŸåï¼‰"""
    
    def __init__(self, chapter_url: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            chapter_url: ç« èŠ‚ URLï¼ˆå¦‚ https://m.anhuagui.com/comic/1128/858078.htmlï¼‰
        """
        # ä¿®å¤åŸŸå
        self.chapter_url = chapter_url.replace('m.anhuagui.com', 'manhuagui.com')
        self.base_url = 'https://m.anhuagui.com'
        self.driver = None
        self.images = []
        
        logger.info(f"ç« èŠ‚URL: {self.chapter_url}")
        logger.info(f"åŸºç¡€URL: {self.base_url}")
        
        self._init_driver()
    
    def _init_driver(self):
        """åˆå§‹åŒ– Chrome WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            logger.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def analyze_page(self):
        """åˆ†æç« èŠ‚é¡µé¢ï¼Œæ‰¾å‡ºæ‰€æœ‰å›¾ç‰‡"""
        try:
            logger.info("å¼€å§‹åˆ†æç« èŠ‚é¡µé¢...")
            self.driver.get(self.chapter_url)
            time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´
            
            page_source = self.driver.page_source
            logger.info(f"é¡µé¢é•¿åº¦: {len(page_source)} å­—ç¬¦")
            
            soup = BeautifulSoup(page_source, 'html.parser')
            images = []
            
            logger.info("æ–¹æ³• 1: æŸ¥æ‰¾ img æ ‡ç­¾...")
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                alt = img.get('alt') or img.get('title') or ''
                
                if src and ('jpg' in src or 'png' in src or 'jpeg' in src or 'webp' in src):
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = urljoin(self.base_url, src)
                    
                    images.append({
                        'url': src,
                        'alt': alt,
                        'tag': 'img'
                    })
                    logger.info(f"  æ‰¾åˆ°å›¾ç‰‡: {alt[:30]}... - {src[:50]}...")
            
            logger.info(f"æ–¹æ³• 1 æ‰¾åˆ° {len(images)} ä¸ª img æ ‡ç­¾")
            
            logger.info("æ–¹æ³• 2: æŸ¥æ‰¾ script æ ‡ç­¾ä¸­çš„å›¾ç‰‡ URL...")
            for script in soup.find_all('script'):
                script_content = script.string
                if script_content:
                    # æŸ¥æ‰¾å›¾ç‰‡æ•°ç»„
                    matches = re.findall(r'["\']([^"\']+\.jpe?g)["\']', script_content)
                    for match in matches:
                        if match.startswith('http'):
                            images.append({
                                'url': match,
                                'alt': f'JS_{len(images) + 1}',
                                'tag': 'script'
                            })
                            logger.info(f"  æ‰¾åˆ°å›¾ç‰‡ (JS): {match[:50]}...")
            
            logger.info(f"æ–¹æ³• 2 æ‰¾åˆ° {len(images) - len([i for i in images if i['tag'] == 'img'])} ä¸ª script å›¾ç‰‡")
            
            # å»é‡
            logger.info("å»é‡å¹¶æ’åº...")
            unique_images = []
            seen_urls = set()
            
            for img in images:
                if img['url'] not in seen_urls:
                    seen_urls.add(img['url'])
                    unique_images.append(img)
            
            self.images = unique_images
            logger.info(f"æ€»å…±æ‰¾åˆ° {len(unique_images)} å¼ å›¾ç‰‡ï¼ˆå»é‡åï¼‰")
            
            return unique_images
        except Exception as e:
            logger.error(f"é¡µé¢åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_image_count_from_page(self):
        """ä»é¡µé¢æ˜¾ç¤ºçš„å›¾ç‰‡æ•°é‡"""
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æŸ¥æ‰¾é¡µé¢æ˜¾ç¤ºçš„å›¾ç‰‡æ•°é‡
            for element in soup.find_all(text=re.compile(r'\d+å¼ |é¡µ|P')):
                text = element.strip()
                if 'å¼ ' in text or 'P' in text:
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        return int(numbers[0])
            
            return len(self.images)
        except:
            return len(self.images)
    
    def download_images(self, save_dir: str, chapter_name: str = "æœªçŸ¥ç« èŠ‚"):
        """ä¸‹è½½æ‰€æœ‰å›¾ç‰‡åˆ°æŒ‡å®šç›®å½•"""
        try:
            logger.info("="*80)
            logger.info(f"å¼€å§‹ä¸‹è½½ {len(self.images)} å¼ å›¾ç‰‡")
            logger.info("="*80)
            logger.info(f"ä¿å­˜ç›®å½•: {save_dir}")
            logger.info(f"ç« èŠ‚åç§°: {chapter_name}")
            logger.info("")
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            Path(save_dir).mkdir(parents=True, exist_ok=True)
            chapter_dir = Path(save_dir) / chapter_name
            chapter_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ç« èŠ‚ç›®å½•åˆ›å»ºæˆåŠŸ: {chapter_dir}")
            logger.info("")
            
            # ä¸‹è½½å›¾ç‰‡
            import requests
            
            success_count = 0
            for i, img in enumerate(self.images, 1):
                img_url = img['url']
                filename = f"{i:03d}.jpg"  # 3 ä½ç¼–å·
                save_path = chapter_dir / filename
                
                logger.info(f"ä¸‹è½½ [{i}/{len(self.images)}] {filename}...")
                
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Referer': self.chapter_url
                    }
                    
                    response = requests.get(img_url, headers=headers, timeout=30, verify=False)
                    if response.status_code == 200:
                        with open(save_path, 'wb') as f:
                            f.write(response.content)
                        success_count += 1
                        logger.info(f"  âœ… ä¸‹è½½æˆåŠŸ")
                    else:
                        logger.warning(f"  âŒ ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    logger.error(f"  âŒ ä¸‹è½½å¼‚å¸¸: {e}")
            
            logger.info("")
            logger.info("="*80)
            logger.info("ä¸‹è½½å®Œæˆ")
            logger.info("="*80)
            logger.info(f"âœ… æˆåŠŸä¸‹è½½: {success_count}/{len(self.images)} å¼ å›¾ç‰‡")
            logger.info(f"ğŸ“ ä¿å­˜ä½ç½®: {chapter_dir}")
            logger.info("="*80)
            
            return success_count
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            logger.info("å…³é—­æµè§ˆå™¨...")
            self.driver.quit()
            self.driver = None


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 analyze_chapter.py <ç« èŠ‚URL> [ä¿å­˜ç›®å½•]")
        print("")
        print("ç¤ºä¾‹:")
        print("  python3 analyze_chapter.py https://m.anhuagui.com/comic/1128/858078.html")
        print("  python3 analyze_chapter.py https://m.anhuagui.com/comic/1128/858078.html downloads")
        print("")
        print("æ³¨æ„:")
        print("  - URL ä¼šè‡ªåŠ¨ä¿®å¤åŸŸåï¼ˆm.anhuagui.com -> manhuagui.comï¼‰")
        print("  - å›¾ç‰‡ä¼šæŒ‰ 001.jpg, 002.jpg... é¡ºåºå‘½å")
        print("  - ä¿å­˜ç›®å½•: downloads/æ¼«ç”»å/ç« èŠ‚å/")
        sys.exit(1)
    
    chapter_url = sys.argv[1]
    save_dir = sys.argv[2] if len(sys.argv) > 2 else "downloads"
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ChapterAnalyzer(chapter_url)
    
    # åˆ†æé¡µé¢
    try:
        analyzer.analyze_page()
        
        # ä¸‹è½½å›¾ç‰‡
        if analyzer.images:
            # ä» URL æå–ç« èŠ‚åç§°
            chapter_num = chapter_url.split('/')[-1].split('.')[0]
            analyzer.download_images(save_dir, f"ç¬¬{chapter_num}è¯")
        else:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè·³è¿‡ä¸‹è½½")
    finally:
        analyzer.close()
        logger.info("âœ… åˆ†æå™¨å·²å…³é—­")
        logger.info("")


if __name__ == "__main__":
    main()
