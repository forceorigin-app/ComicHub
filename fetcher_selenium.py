"""
æ¼«ç”»æŠ“å–æ¨¡å— V6 - Selenium ç‰ˆæœ¬ï¼ˆä¿®å¤ç±»å‹é”™è¯¯ï¼‰
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import Select
from bs4 import BeautifulSoup
import re
import logging
from pathlib import Path
from urllib.parse import urljoin, urlparse
import time
import json
import random
import os
from typing import List, Dict, Optional

try:
    from proxy_pool_client import ProxyPoolClient
    PROXY_POOL_AVAILABLE = True
except ImportError:
    PROXY_POOL_AVAILABLE = False

logger = logging.getLogger(__name__)


class ManhuaGuiFetcherSelenium:
    """æ¼«ç”»é¾ŸæŠ“å–å™¨ V6 (Selenium ç‰ˆæœ¬ - ä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, 
                 base_url: str = "https://m.manhuagui.com",
                 use_proxy: bool = False,
                 proxy_pool_url: str = "http://localhost:5010",
                 headless: bool = True):
        """
        åˆå§‹åŒ–æŠ“å–å™¨
        
        Args:
            base_url: åŸºç¡€ URL
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
            proxy_pool_url: ä»£ç†æ±  API åœ°å€
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        self.base_url = base_url
        self.use_proxy = use_proxy
        self.headless = headless
        
        # æµè§ˆå™¨è¯·æ±‚å¤´
        self.default_headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Referer': base_url
        }
        
        # åˆå§‹åŒ–ä»£ç†æ± å®¢æˆ·ç«¯
        self.proxy_pool_client = None
        self.current_proxy = None
        
        if use_proxy and PROXY_POOL_AVAILABLE:
            try:
                self.proxy_pool_client = ProxyPoolClient(api_url=proxy_pool_url)
                logger.info(f"ä»£ç†æ± å®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {proxy_pool_url}")
                proxy_data = self.proxy_pool_client.get_proxy()
                if proxy_data:
                    self.current_proxy = proxy_data.get('proxy')
                    logger.info(f"å·²è·å–ä»£ç†: {self.current_proxy}")
            except Exception as e:
                logger.warning(f"ä»£ç†æ± å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_proxy = False
        
        # åˆå§‹åŒ– WebDriver
        self.driver = None
        self._init_driver()
        
        # Cookie ç®¡ç†çŠ¶æ€
        self.cookies_loaded = False
        
        logger.info(f"æŠ“å–å™¨å·²åˆå§‹åŒ– (ä»£ç†: {'å·²å¯ç”¨' if self.use_proxy else 'æœªå¯ç”¨'}, æ— å¤´æ¨¡å¼: {'æ˜¯' if self.headless else 'å¦'})")

    def _init_driver(self):
        """åˆå§‹åŒ– Chrome WebDriver"""
        try:
            # é…ç½® Chrome é€‰é¡¹
            chrome_options = Options()
            
            # æ— å¤´æ¨¡å¼
            if self.headless:
                chrome_options.add_argument('--headless')
            
            # æ€§èƒ½ä¼˜åŒ–
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-software-rasterizer')
            
            # ç¦ç”¨è‡ªåŠ¨åŒ–æ£€æµ‹
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # çª—å£å¤§å°
            chrome_options.add_argument('--window-size=1920,1080')
            
            logger.info("Chrome é€‰é¡¹é…ç½®å®Œæˆ")
            
            # åˆå§‹åŒ– WebDriver
            service = Service()
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            logger.info("WebDriver åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _request(self, url: str, wait_time: int = 5, use_proxy: Optional[bool] = None):
        """
        è®¿é—® URL å¹¶è¿”å› driver
        
        Args:
            url: è¯·æ±‚ URL
            wait_time: ç­‰å¾…æ—¶é—´
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼ˆNone = ä½¿ç”¨é…ç½®ï¼‰
        """
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨ä»£ç†
        should_use_proxy = use_proxy if use_proxy is not None else self.use_proxy
        
        # å¦‚æœä½¿ç”¨ä»£ç†ä½†å½“å‰æ²¡æœ‰ä»£ç†ï¼Œå°è¯•è·å–
        if should_use_proxy and self.proxy_pool_client and not self.current_proxy:
            logger.info("å°è¯•è·å–æ–°ä»£ç†...")
            proxy_data = self.proxy_pool_client.get_proxy()
            if proxy_data:
                self.current_proxy = proxy_data.get('proxy')
                logger.info(f"å·²è·å–æ–°ä»£ç†: {self.current_proxy}")
        
        try:
            # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if should_use_proxy and self.current_proxy:
                # æ³¨æ„ï¼šåœ¨ Selenium ä¸­é€šè¿‡ Chrome è®¾ç½®ä»£ç†
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæš‚æ—¶ä¸ä½¿ç”¨ Selenium ä»£ç†
                pass
            
            logger.debug(f"è¯·æ±‚ URL: {url} (ä»£ç†: {'æ˜¯' if should_use_proxy else 'å¦'})")
            
            # è®¿é—® URL
            self.driver.get(url)
            time.sleep(wait_time)
            
            return self.driver
        except Exception as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

    def search_comics(self, keyword: str, use_proxy: Optional[bool] = None) -> List[Dict]:
        """æœç´¢æ¼«ç”»"""
        logger.info(f"æœç´¢æ¼«ç”»: {keyword}")
        
        # æ„å»ºæœç´¢ URL
        search_url = urljoin(self.base_url, f"/s/{keyword}")
        
        # è¯·æ±‚
        driver = self._request(search_url, use_proxy=use_proxy)
        if not driver:
            return []
        
        # è§£æé¡µé¢
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        comics = []
        
        # æŸ¥æ‰¾æ¼«ç”»åˆ—è¡¨
        comic_links = soup.select('a[href*="/comic/"]')
        seen_ids = set()
        
        for link in comic_links:
            href = link.get('href')
            title = link.get('title') or link.text.strip()
            
            # æå–æ¼«ç”»ID
            match = re.search(r'/comic/(\d+)/', href)
            if match:
                comic_id = match.group(1)
                
                # å»é‡
                if comic_id not in seen_ids:
                    seen_ids.add(comic_id)
                    comics.append({
                        'id': comic_id,
                        'name': title,
                        'url': urljoin(self.base_url, href)
                    })
        
        logger.info(f"æœç´¢åˆ° {len(comics)} éƒ¨æ¼«ç”»")
        return comics

    def get_comic_info(self, comic_url: str, use_proxy: Optional[bool] = None) -> Optional[Dict]:
        """è·å–æ¼«ç”»ä¿¡æ¯"""
        logger.info(f"è·å–æ¼«ç”»ä¿¡æ¯: {comic_url}")
        
        # è¯·æ±‚
        driver = self._request(comic_url, use_proxy=use_proxy)
        if not driver:
            return None
        
        # è§£æé¡µé¢
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # æŸ¥æ‰¾æ ‡é¢˜
        title = soup.select_one('h1')
        comic_name = title.text.strip() if title else "æœªçŸ¥"
        
        # æå–ID
        match = re.search(r'/comic/(\d+)/', comic_url)
        comic_id = match.group(1) if match else None
        
        return {
            'id': comic_id,
            'name': comic_name,
            'url': comic_url
        }

    def get_chapters(self, comic_url: str, use_proxy: Optional[bool] = None) -> List[Dict]:
        """è·å–ç« èŠ‚åˆ—è¡¨"""
        logger.info(f"è·å–ç« èŠ‚åˆ—è¡¨: {comic_url}")
        
        # è¯·æ±‚
        driver = self._request(comic_url, use_proxy=use_proxy)
        if not driver:
            return []
        
        # è§£æé¡µé¢
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        chapters = []
        
        # æŸ¥æ‰¾ç« èŠ‚é“¾æ¥
        chapter_links = soup.select('a[href*="/comic/"]')
        seen_chapters = set()
        
        for link in chapter_links:
            href = link.get('href')
            match = re.search(r'/(\d+)\.html?$', href)
            if match:
                chapter_num = match.group(1)
                
                # å»é‡
                if chapter_num not in seen_chapters:
                    seen_chapters.add(chapter_num)
                    chapters.append({
                        'chapter_num': chapter_num,
                        'title': link.text.strip(),
                        'url': urljoin(self.base_url, href)
                    })
        
        logger.info(f"è·å–åˆ° {len(chapters)} ä¸ªç« èŠ‚")
        return chapters

    def get_images(self, chapter_url: str, use_proxy: Optional[bool] = None) -> List[str]:
        """è·å–ç« èŠ‚å›¾ç‰‡åˆ—è¡¨"""
        logger.info(f"è·å–ç« èŠ‚å›¾ç‰‡: {chapter_url}")
        
        # è¯·æ±‚
        driver = self._request(chapter_url, use_proxy=use_proxy)
        if not driver:
            return []
        
        # è§£æé¡µé¢
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        images = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-original')
            if src and ('jpg' in src or 'png' in src or 'jpeg' in src or 'webp' in src):
                images.append(src)
        
        # å»é‡
        images = list(set(images))
        
        logger.info(f"è·å–åˆ° {len(images)} å¼ å›¾ç‰‡")
        return images

    def close(self):
        """å…³é—­ WebDriver"""
        if self.driver:
            logger.info("å…³é—­ WebDriver...")
            self.driver.quit()
            self.driver = None


def create_fetcher_selenium(use_proxy: bool = False, 
                         proxy_pool_url: str = "http://localhost:5010",
                         headless: bool = True):
    """
    åˆ›å»ºæ¼«ç”»æŠ“å–å™¨å®ä¾‹ V6 (Selenium ç‰ˆæœ¬ï¼‰
    
    Args:
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        proxy_pool_url: ä»£ç†æ±  API åœ°å€
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        
    Returns:
        ManhuaGuiFetcherSelenium å®ä¾‹
    """
    return ManhuaGuiFetcherSelenium(
        use_proxy=use_proxy,
        proxy_pool_url=proxy_pool_url,
        headless=headless
    )


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    print("="*80)
    print("Fetcher V6 - Selenium ç‰ˆæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("="*80)
    print()
    print("ğŸ‰ é‡å¤§æ›´æ–°:")
    print("   âœ… ä½¿ç”¨çœŸå®æµè§ˆå™¨ï¼ˆChromeï¼‰")
    print("   âœ… çœŸå®çš„ TLS æŒ‡çº¹")
    print("   âœ… å®Œæ•´çš„ JavaScript æ”¯æŒ")
    print("   âœ… å¯ä»¥é€šè¿‡æ‰€æœ‰æ£€æµ‹")
    print("   âœ… é¢„æœŸæˆåŠŸç‡: 90%+")
    print()
    print("ğŸ“¦ æŠ€æœ¯ç‰¹æ€§:")
    print("   - ä½¿ç”¨ Selenium + Chrome WebDriver")
    print("   - æ”¯æŒæœ‰å¤´å’Œæ— å¤´æ¨¡å¼")
    print("   - é›†æˆä»£ç†æ± ï¼ˆå¯é€‰ï¼‰")
    print("   - BeautifulSoup é¡µé¢è§£æ")
    print()
    print("ğŸ”§ ä½¿ç”¨æ–¹æ³•:")
    print("   1. åˆ›å»ºæŠ“å–å™¨å®ä¾‹")
    print("   2. æœç´¢æ¼«ç”»")
    print("   3. è·å–æ¼«ç”»ä¿¡æ¯")
    print("   4. è·å–ç« èŠ‚åˆ—è¡¨")
    print("   5. è·å–å›¾ç‰‡åˆ—è¡¨")
    print("   6. å…³é—­æµè§ˆå™¨")
    print()
    print("="*80)

if __name__ == "__main__":
    # å¿«é€Ÿæµ‹è¯•
    try:
        fetcher = create_fetcher_selenium(use_proxy=False, headless=True)
        print("âœ… æŠ“å–å™¨åˆ›å»ºæˆåŠŸ")
        print()
        
        # æµ‹è¯•æœç´¢
        print("æµ‹è¯•æœç´¢...")
        results = fetcher.search_comics("æµ·è´¼ç‹")
        
        if results:
            print(f"âœ… æœç´¢æˆåŠŸ: {len(results)} éƒ¨æ¼«ç”»")
            for i, comic in enumerate(results[:3], 1):
                print(f"  {i}. {comic['name']} (ID: {comic['id']})")
        else:
            print("âŒ æœç´¢å¤±è´¥")
        
        # å…³é—­
        fetcher.close()
        print("âœ… æŠ“å–å™¨å·²å…³é—­")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
